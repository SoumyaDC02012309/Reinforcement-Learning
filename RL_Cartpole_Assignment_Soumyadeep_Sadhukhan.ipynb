{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1701784412034,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "51KifDZWsLW1",
    "outputId": "a414ecb2-761c-44b6-b387-b35205281a9e"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701784412559,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "a9EkIDeRsN-1",
    "outputId": "dc399814-0e35-4cfe-fc05-60d3a7b82e4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701784412559,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "lT9ndewBsP0z",
    "outputId": "c19f897d-302f-4120-edaa-6a0c09a2d5da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701784412559,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "CboYy1gaOy6S"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticPolicy:\n",
    "\n",
    "    def __init__(self, θ, α, γ):\n",
    "        # Initialize paramters θ, learning rate α and discount factor γ\n",
    "\n",
    "        self.θ = θ\n",
    "        self.α = α\n",
    "        self.γ = γ\n",
    "\n",
    "    def logistic(self, y):\n",
    "        # definition of logistic function\n",
    "\n",
    "        return 1/(1 + np.exp(-y))\n",
    "\n",
    "    def probs(self, x):\n",
    "        # returns probabilities of two actions\n",
    "\n",
    "        y = x @ self.θ\n",
    "        prob0 = self.logistic(y)\n",
    "\n",
    "        return np.array([prob0, 1-prob0])\n",
    "\n",
    "    def act(self, x):\n",
    "        # sample an action in proportion to probabilities\n",
    "\n",
    "        probs = self.probs(x)\n",
    "        action = np.random.choice([0, 1], p=probs)\n",
    "\n",
    "        return action, probs[action]\n",
    "\n",
    "    def grad_log_p(self, x):\n",
    "        # calculate grad-log-probs\n",
    "\n",
    "        y = x @ self.θ\n",
    "        grad_log_p0 = x - x*self.logistic(y)\n",
    "        grad_log_p1 = - x*self.logistic(y)\n",
    "\n",
    "        return grad_log_p0, grad_log_p1\n",
    "\n",
    "    def grad_log_p_dot_rewards(self, grad_log_p, actions, discounted_rewards):\n",
    "        # dot grads with future rewards for each action in episode\n",
    "\n",
    "        return grad_log_p.T @ discounted_rewards\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        # calculate temporally adjusted, discounted rewards\n",
    "\n",
    "        discounted_rewards = np.zeros(len(rewards))\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(0, len(rewards))):\n",
    "            cumulative_rewards = cumulative_rewards * self.γ + rewards[i]\n",
    "            discounted_rewards[i] = cumulative_rewards\n",
    "\n",
    "        return discounted_rewards\n",
    "\n",
    "    def update(self, rewards, obs, actions):\n",
    "        # calculate gradients for each action over all observations\n",
    "        grad_log_p = np.array([self.grad_log_p(ob)[action] for ob,action in zip(obs,actions)])\n",
    "\n",
    "        assert grad_log_p.shape == (len(obs), 4)\n",
    "\n",
    "        # calculate temporaly adjusted, discounted rewards\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "\n",
    "        # gradients times rewards\n",
    "        dot = self.grad_log_p_dot_rewards(grad_log_p, actions, discounted_rewards)\n",
    "\n",
    "        # gradient ascent on parameters\n",
    "        self.θ += self.α*dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701784412560,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "z38R-B5nPZOH"
   },
   "outputs": [],
   "source": [
    "def run_episode(env, policy, render=False):\n",
    "\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        observations.append(observation)\n",
    "\n",
    "        action, prob = policy.act(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        totalreward += reward\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        probs.append(prob)\n",
    "\n",
    "    return totalreward, np.array(rewards), np.array(observations), np.array(actions), np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701784412560,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "HbiuAza4PaUP"
   },
   "outputs": [],
   "source": [
    "def train(θ, α, γ, Policy, MAX_EPISODES=1000, seed=None, evaluate=False):\n",
    "\n",
    "    # initialize environment and policy\n",
    "    env = gym.make('CartPole-v1')\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    episode_rewards = []\n",
    "    policy = Policy(θ, α, γ)\n",
    "\n",
    "    # train until MAX_EPISODES\n",
    "    for i in range(MAX_EPISODES):\n",
    "\n",
    "        # run a single episode\n",
    "        total_reward, rewards, observations, actions, probs = run_episode(env, policy)\n",
    "\n",
    "        # keep track of episode rewards\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # update policy\n",
    "        policy.update(rewards, observations, actions)\n",
    "        print(\"EP: \" + str(i) + \" Score: \" + str(total_reward) + \" \",end=\"\\r\", flush=False)\n",
    "\n",
    "    # evaluation call after training is finished - evaluate last trained policy on 100 episodes\n",
    "    if evaluate:\n",
    "        env = RecordVideo(env, 'pg_cartpole/')\n",
    "        for _ in range(100):\n",
    "            run_episode(env, policy, render=False)\n",
    "        env.env.close()\n",
    "\n",
    "    return episode_rewards, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74557,
     "status": "ok",
     "timestamp": 1701784487112,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "Buehrd80Pa1p",
    "outputId": "829b2bd1-8bcb-4d08-9a39-b2eec1fc6047"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SOUMYADEEP SADHUKHAN\\AppData\\Local\\Temp\\ipykernel_22092\\192022845.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = x @ self.θ\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m GLOBAL_SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(GLOBAL_SEED)\n\u001b[1;32m----> 8\u001b[0m episode_rewards, policy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mθ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mα\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.002\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mγ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mPolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLogisticPolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mMAX_EPISODES\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(θ, α, γ, Policy, MAX_EPISODES, seed, evaluate)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# train until MAX_EPISODES\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_EPISODES):\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# run a single episode\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     total_reward, rewards, observations, actions, probs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# keep track of episode rewards\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     episode_rewards\u001b[38;5;241m.\u001b[39mappend(total_reward)\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mrun_episode\u001b[1;34m(env, policy, render)\u001b[0m\n\u001b[0;32m     15\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     17\u001b[0m observations\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[1;32m---> 19\u001b[0m action, prob \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     22\u001b[0m totalreward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36mLogisticPolicy.act\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# sample an action in proportion to probabilities\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], p\u001b[38;5;241m=\u001b[39mprobs)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action, probs[action]\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mLogisticPolicy.probs\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprobs\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# returns probabilities of two actions\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mθ\u001b[49m\n\u001b[0;32m     21\u001b[0m     prob0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogistic(y)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([prob0, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mprob0])\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 2)"
     ]
    }
   ],
   "source": [
    "# additional imports for saving and loading a trained policy\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "# for reproducibility\n",
    "GLOBAL_SEED = 0\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "episode_rewards, policy = train(θ=np.random.rand(4),\n",
    "                                α=0.002,\n",
    "                                γ=0.99,\n",
    "                                Policy=LogisticPolicy,\n",
    "                                MAX_EPISODES=2000,\n",
    "                                seed= None,\n",
    "                                evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1701784487119,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "FiCOEC9DPz_h",
    "outputId": "35f971af-d9c2-43cf-861d-f0b2016ea2dd"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab import files\n",
    "\n",
    "plt.plot(episode_rewards);\n",
    "plt.savefig('Cartpole_Main.png')\n",
    "#files.download('Cartpole_Main.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1701784487949,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "JjmQYIOJP5Cl",
    "outputId": "df8046ae-3186-4aa0-a72c-270ecef9e921"
   },
   "outputs": [],
   "source": [
    "np.var(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701784487949,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "W9kTaKJEsSs1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticPolicy:\n",
    "\n",
    "    def __init__(self, θ, α, γ):\n",
    "        # Initialize paramters θ, learning rate α and discount factor γ\n",
    "\n",
    "        self.θ = θ\n",
    "        self.α = α\n",
    "        self.γ = γ\n",
    "\n",
    "    def logistic(self, y):\n",
    "        # definition of logistic function\n",
    "\n",
    "        return 1/(1 + np.exp(-y))\n",
    "\n",
    "    def probs(self, x):\n",
    "        # returns probabilities of two actions\n",
    "\n",
    "        y = x @ self.θ\n",
    "        prob0 = self.logistic(y)\n",
    "\n",
    "        return np.array([prob0, 1-prob0])\n",
    "\n",
    "    def act(self, x):\n",
    "        # sample an action in proportion to probabilities\n",
    "\n",
    "        probs = self.probs(x)\n",
    "        action = np.random.choice([0, 1], p=probs)\n",
    "\n",
    "        return action, probs[action]\n",
    "\n",
    "    def grad_log_p(self, x):\n",
    "        # calculate grad-log-probs\n",
    "\n",
    "        y = x @ self.θ\n",
    "        grad_log_p0 = x - x*self.logistic(y)\n",
    "        grad_log_p1 = - x*self.logistic(y)\n",
    "\n",
    "        return grad_log_p0, grad_log_p1\n",
    "\n",
    "    \"\"\"def baseline(self, rewards):\n",
    "        # calculate baseline\n",
    "        baseline = np.mean(rewards)\n",
    "        return baseline\"\"\"\n",
    "\n",
    "    \"\"\"def baseline(self, rewards):\n",
    "        baseline = np.cumsum(rewards) / (np.arange(len(rewards)) + 1)\n",
    "        return baseline\"\"\"\n",
    "\n",
    "    \"\"\"def td_error_baseline(self, rewards):\n",
    "        baseline = np.zeros_like(rewards)\n",
    "        for i in range(len(rewards) - 1, 0, -1):\n",
    "            baseline[i] = baseline[i + 1] + rewards[i] - self.γ * rewards[i + 1]\n",
    "        return baseline\"\"\"\n",
    "\n",
    "\n",
    "    def td_error_baseline(self, rewards):\n",
    "        baseline = np.zeros_like(rewards)\n",
    "        bonus = 0.1 * rewards  # Simple bonus function\n",
    "\n",
    "        for i in range(len(rewards) - 1, -1, -1):\n",
    "            if i == len(rewards) - 1:\n",
    "                baseline[i] = rewards[i] + bonus[i]  # Handle the last state separately\n",
    "            else:\n",
    "                baseline[i] = baseline[i + 1] + rewards[i] - self.γ * rewards[i + 1] + bonus[i]\n",
    "\n",
    "        return baseline\n",
    "\n",
    "    \"\"\"def td_error_baseline(self, rewards):\n",
    "        baseline = np.zeros_like(rewards)\n",
    "        for i in range(len(rewards) - 1, -1, -1):\n",
    "            if i == len(rewards) - 1:\n",
    "                baseline[i] = rewards[i]  # Handle the last state separately\n",
    "            else:\n",
    "                baseline[i] = baseline[i + 1] + rewards[i] - self.γ * rewards[i + 1]\n",
    "\n",
    "        return baseline\"\"\"\n",
    "\n",
    "    \"\"\"def monte_carlo_baseline(self, rewards, num_simulations = 3):\n",
    "        baseline = np.zeros_like(rewards)\n",
    "\n",
    "        for i in range(len(rewards)):\n",
    "            simulated_rewards = []\n",
    "            for _ in range(num_simulations):\n",
    "                simulated_rewards.append(np.sum(rewards[i:]))\n",
    "\n",
    "            baseline[i] = np.mean(simulated_rewards)\n",
    "\n",
    "        return baseline\"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"def running_average_baseline(self, rewards, alpha=0.9):\n",
    "        baseline = np.zeros_like(rewards)\n",
    "        for i in range(len(rewards)):\n",
    "            baseline[i] = alpha * baseline[i - 1] + (1 - alpha) * rewards[i]\n",
    "        return baseline\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def grad_log_p_dot_rewards(self, grad_log_p, actions, discounted_rewards):\n",
    "        # dot grads with future rewards for each action in episode\\\n",
    "        #baseline = self.baseline(discounted_rewards)\n",
    "        #discounted_rewards = discounted_rewards - baseline\n",
    "\n",
    "        return grad_log_p.T @ discounted_rewards\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        # calculate temporally adjusted, discounted rewards\n",
    "\n",
    "        baseline = self.td_error_baseline(rewards)\n",
    "        discounted_rewards = np.zeros(len(rewards))\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(0, len(rewards))):\n",
    "            cumulative_rewards = cumulative_rewards * self.γ + rewards[i]\n",
    "            discounted_rewards[i] = cumulative_rewards - baseline[i]\n",
    "\n",
    "        return discounted_rewards\n",
    "\n",
    "\n",
    "    def update(self, rewards, obs, actions):\n",
    "        # calculate gradients for each action over all observations\n",
    "        grad_log_p = np.array([self.grad_log_p(ob)[action] for ob,action in zip(obs,actions)])\n",
    "\n",
    "        assert grad_log_p.shape == (len(obs), 4)\n",
    "\n",
    "        # calculate temporaly adjusted, discounted rewards\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "\n",
    "        # gradients times rewards\n",
    "        dot = self.grad_log_p_dot_rewards(grad_log_p, actions, discounted_rewards)\n",
    "\n",
    "        # gradient ascent on parameters\n",
    "        self.θ += self.α*dot\n",
    "\n",
    "\n",
    "    def get_learning_rate(self, episode_num):\n",
    "        return self.α / (1 + episode_num * 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701784487949,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "LMSNEVEhslIP"
   },
   "outputs": [],
   "source": [
    "def run_episode(env, policy, render=False):\n",
    "\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    probs = []\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        observations.append(observation)\n",
    "\n",
    "        action, prob = policy.act(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        totalreward += reward\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        probs.append(prob)\n",
    "\n",
    "    return totalreward, np.array(rewards), np.array(observations), np.array(actions), np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701784487949,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "FUMTsucQstJj"
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "def train(θ, α, γ, Policy, MAX_EPISODES=1000, seed=None, evaluate=False):\n",
    "\n",
    "    # initialize environment and policy\n",
    "    env = gym.make('CartPole-v1')\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    episode_rewards = []\n",
    "    policy = Policy(θ, α, γ)\n",
    "\n",
    "    # train until MAX_EPISODES\n",
    "    for i in range(MAX_EPISODES):\n",
    "\n",
    "        # run a single episode\n",
    "        total_reward, rewards, observations, actions, probs = run_episode(env, policy)\n",
    "\n",
    "        # keep track of episode rewards\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # update policy\n",
    "        policy.update(rewards, observations, actions)\n",
    "        print(\"EP: \" + str(i) + \" Score: \" + str(total_reward) + \" \",end=\"\\r\", flush=False)\n",
    "\n",
    "    # evaluation call after training is finished - evaluate last trained policy on 100 episodes\n",
    "    if evaluate:\n",
    "        env = RecordVideo(env, 'pg_cartpole/')\n",
    "        for _ in range(100):\n",
    "            run_episode(env, policy, render=False)\n",
    "        env.env.close()\n",
    "\n",
    "    return episode_rewards, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78616,
     "status": "ok",
     "timestamp": 1701784566561,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "IBCTKo8Rsv0D",
    "outputId": "0dffc2db-87c4-4085-c2ed-b9251719dbda"
   },
   "outputs": [],
   "source": [
    "# additional imports for saving and loading a trained policy\n",
    "\n",
    "\n",
    "# for reproducibility\n",
    "GLOBAL_SEED = 0\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "episode_rewards, policy = train(θ=np.random.rand(4),\n",
    "                                α=0.002,\n",
    "                                γ=0.99,\n",
    "                                Policy=LogisticPolicy,\n",
    "                                MAX_EPISODES=2000,\n",
    "                                seed=None,\n",
    "                                #num_simulations = 4,\n",
    "                                evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1701784566561,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "qhkrU_jMsx2S",
    "outputId": "53174cb4-bbef-4511-aed4-9fd7628ef4e7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab import files\n",
    "\n",
    "plt.plot(episode_rewards);\n",
    "plt.savefig('Cartpole_TD.png')\n",
    "#files.download('Cartpole_TD.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1082,
     "status": "ok",
     "timestamp": 1701784567633,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "dS3XSOCQATqe",
    "outputId": "ddaed26b-c984-4bbf-f9ec-776ec6a25ab9"
   },
   "outputs": [],
   "source": [
    "np.var(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 82774,
     "status": "ok",
     "timestamp": 1701784650404,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "E5lMzOc3AXLJ",
    "outputId": "e3bc3237-a9e2-472f-b406-c707940f2e33"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym.wrappers import RecordVideo\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab import files\n",
    "\n",
    "class LogisticPolicy:\n",
    "    def __init__(self, θ, α, γ):\n",
    "        self.θ = θ\n",
    "        self.α = α\n",
    "        self.γ = γ\n",
    "\n",
    "    def logistic(self, y):\n",
    "        return 1 / (1 + np.exp(-y))\n",
    "\n",
    "    def probs(self, x):\n",
    "        y = x @ self.θ\n",
    "        prob0 = self.logistic(y)\n",
    "        return np.array([prob0, 1 - prob0])\n",
    "\n",
    "    def act(self, x):\n",
    "        probs = self.probs(x)\n",
    "        action = np.random.choice([0, 1], p=probs)\n",
    "        return action, probs[action]\n",
    "\n",
    "    def grad_log_p(self, x):\n",
    "        y = x @ self.θ\n",
    "        grad_log_p0 = x - x * self.logistic(y)\n",
    "        grad_log_p1 = -x * self.logistic(y)\n",
    "        return grad_log_p0, grad_log_p1\n",
    "\n",
    "    def grad_log_p_dot_rewards(self, grad_log_p, actions, discounted_rewards):\n",
    "        return grad_log_p.T @ discounted_rewards\n",
    "\n",
    "    def calculate_baseline(self, rewards):\n",
    "        baseline = np.cumsum(rewards) / (np.arange(len(rewards)) + 1)\n",
    "        return baseline\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros(len(rewards))\n",
    "        cumulative_rewards = 0\n",
    "        baseline = self.calculate_baseline(rewards)\n",
    "        for i in reversed(range(0, len(rewards))):\n",
    "            cumulative_rewards = cumulative_rewards * self.γ + rewards[i]\n",
    "            discounted_rewards[i] = cumulative_rewards - baseline[i]\n",
    "        return discounted_rewards\n",
    "\n",
    "    def update(self, rewards, obs, actions):\n",
    "        grad_log_p = np.array([self.grad_log_p(ob)[action] for ob, action in zip(obs, actions)])\n",
    "        assert grad_log_p.shape == (len(obs), 4)\n",
    "\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "        #baseline = self.calculate_baseline(rewards)\n",
    "        advantages = discounted_rewards\n",
    "\n",
    "        dot = self.grad_log_p_dot_rewards(grad_log_p, actions, advantages)\n",
    "\n",
    "        self.θ += self.α * dot\n",
    "\n",
    "def run_episode(env, policy, render=False):\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    observations, actions, rewards, probs = [], [], [], []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        observations.append(observation)\n",
    "        action, prob = policy.act(observation)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        totalreward += reward\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        probs.append(prob)\n",
    "\n",
    "    return totalreward, np.array(rewards), np.array(observations), np.array(actions), np.array(probs)\n",
    "\n",
    "def train(θ, α, γ, Policy, MAX_EPISODES=5000, seed=None, evaluate=False):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    episode_rewards = []\n",
    "    policy = Policy(θ, α, γ)\n",
    "\n",
    "    for i in range(MAX_EPISODES):\n",
    "        total_reward, rewards, observations, actions, probs = run_episode(env, policy)\n",
    "        episode_rewards.append(total_reward)\n",
    "        policy.update(rewards, observations, actions)\n",
    "        print(\"EP: \" + str(i) + \" Score: \" + str(total_reward) + \" \", end=\"\\r\", flush=False)\n",
    "\n",
    "    if evaluate:\n",
    "        env = RecordVideo(env, 'pg_cartpole/')\n",
    "        for _ in range(100):\n",
    "            run_episode(env, policy, render=False)\n",
    "        env.env.close()\n",
    "\n",
    "    return episode_rewards, policy\n",
    "\n",
    "# For reproducibility\n",
    "GLOBAL_SEED = 0\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "episode_rewards, policy = train(θ=np.random.rand(4),\n",
    "                                α=0.002,\n",
    "                                γ=0.99,\n",
    "                                Policy=LogisticPolicy,\n",
    "                                MAX_EPISODES=2000,\n",
    "                                seed=None,\n",
    "                                evaluate=True)\n",
    "\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training Progress')\n",
    "plt.show()\n",
    "plt.savefig('Cartpole_Cumsum.png')\n",
    "#files.download('Cartpole_Cumsum.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1701784650404,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "GSIr7NtRDajA",
    "outputId": "f32bbb27-bdcf-4326-93f5-dff47f3f9ed5"
   },
   "outputs": [],
   "source": [
    "np.var(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1701784650404,
     "user": {
      "displayName": "Soumyadeep Sadhukhan",
      "userId": "10934723042522895248"
     },
     "user_tz": -330
    },
    "id": "r7gxdy2qowU_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
